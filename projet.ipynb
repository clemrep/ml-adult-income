{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b70c21d4",
   "metadata": {},
   "source": [
    "# Adult Income Classification Project\n",
    "\n",
    "Le projet vise à analyser le jeu de données Adult Income afin d’identifier les différents facteurs qui influencent le niveau de revenu d'une personne et de construire un modèle capable de prédire si une personne gagne plus ou moins de 50 000 dollars par an. \n",
    "\n",
    "Le dataset, disponible dans plusieurs versions (UCI, OpenML, variantes nettoyées ou simplifiées), est ici utilisé dans sa version OpenML, qui offre un format homogène et des variables à la fois numériques et catégorielles. Notre problème sera approché sous la forme d'une classification binaire (+ ou - de 50k$/an de revenus) car le dataset ne contient pas la valeur exacte du revenu, qu'une catégorie.\n",
    "\n",
    "Ce notebook sera articulé en trois grandes parties :\n",
    "\n",
    "1. **Exploration des données (EDA) et analyses non supervisées**\n",
    "2. **Préprocessing des données et séparation des jeux d'apprentissage/test**\n",
    "3. **Modélisation supervisée (baseline vs ensembles) et comparaison des performances**\n",
    "\n",
    "Chaque section inclut des explications détaillées pour assurer la reproductibilité de l'analyse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0924681d",
   "metadata": {},
   "source": [
    "Avant de commencer, voici tous les imports de librairie dont nous auront besoin pour lancer les cellules ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b62386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", context=\"notebook\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 5)\n",
    "\n",
    "DATA_PATH = \"adult.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e0139b",
   "metadata": {},
   "source": [
    "# 1 - Exploration de données et analyse non supervisée"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d870bb3c",
   "metadata": {},
   "source": [
    "## Caractéristiques du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cf39cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9283f935",
   "metadata": {},
   "source": [
    "**Nombre d'instances :** Le dataset complet contient 48 842 entrées. C'est un volume suffisant pour entraîner un modèle robuste.\n",
    "\n",
    "**Nombre de features :** Il y a 14 variables explicatives + 1 variable cible (income)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5310451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTypes de données:\")\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69747d21",
   "metadata": {},
   "source": [
    "**Types de variables :**\n",
    "\n",
    "- **Numériques :** age, fnlwgt (= final weight), education-num, capital-gain, capital-loss, hours-per-week.\n",
    "\n",
    "- **Catégorielles :** workclass, education, marital-status, occupation, relationship, race, sex, native-country."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd8a04e",
   "metadata": {},
   "source": [
    "## Analyse et gestion des valeurs manquantes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3068427",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nValeurs manquantes par colonne:\")\n",
    "missing_counts = df.isna().sum()\n",
    "print(missing_counts[missing_counts > 0] if missing_counts.sum() > 0 else \"Aucune valeur manquante détectée\")\n",
    "\n",
    "# Détecter les '?' comme valeurs manquantes\n",
    "print(\"\\n--- Détection des '?' comme valeurs manquantes ---\")\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        question_count = (df[col] == '?').sum()\n",
    "        if question_count > 0:\n",
    "            print(f\"{col}: {question_count} valeurs manquantes ('?')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d60501a",
   "metadata": {},
   "source": [
    "\n",
    "### Interprétation des valeurs manquantes\n",
    "\n",
    "Les points d'interrogation ('?') observés dans le dataset représentent des **valeurs manquantes** (données non disponibles ou non renseignées). Ces valeurs peuvent affecter la qualité de nos prédictions et doivent être traitées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7686ec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace('?', np.nan)\n",
    "\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Colonne': df.columns,\n",
    "    'Manquantes': df.isna().sum(),\n",
    "    'Pourcentage': (df.isna().sum() / len(df) * 100).round(2)\n",
    "})\n",
    "missing_summary = missing_summary[missing_summary['Manquantes'] > 0].sort_values('Manquantes', ascending=False)\n",
    "print(missing_summary.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d881446",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df.copy()\n",
    "df_temp = df_temp.replace('?', np.nan)\n",
    "rows_with_missing = df_temp.isna().any(axis=1).sum()\n",
    "\n",
    "total_rows = len(df)\n",
    "percentage_lost = (rows_with_missing / total_rows) * 100\n",
    "\n",
    "print(f\"   - Nombre total de lignes: {total_rows:,}\")\n",
    "print(f\"   - Lignes contenant au moins un '?': {rows_with_missing:,}\")\n",
    "print(f\"   - Pourcentage de données PERDUES: {percentage_lost:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44f8f46",
   "metadata": {},
   "source": [
    "Notre analyse préliminaire montre que la suppression des lignes contenant au moins une valeur manquante entraînerait une perte de données supérieure à 5%. Nous craignons que réduire la taille du jeu d'entraînement d'autant de valeurs puisse nuire à la capacité du modèle à généraliser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20352062",
   "metadata": {},
   "source": [
    "### Stratégie choisie pour ce projet\n",
    "\n",
    "Nous n'avons que des variables catégorielles à imputer, nous avons donc fait le choix de faire une imputation par mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f5c5b4",
   "metadata": {},
   "source": [
    "# Répartition des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16dc259",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(data=df, x=\"income\")\n",
    "plt.title(\"Répartition de la variable cible (income)\")\n",
    "plt.show()\n",
    "\n",
    "income_ratio = df[\"income\"].value_counts(normalize=True)\n",
    "print(\"Répartition proportionnelle:\\n\", income_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0da26de",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "sns.boxplot(data=df, x=\"income\", y=\"age\", ax=axes[0])\n",
    "axes[0].set_title(\"Distribution de l'âge selon le revenu\")\n",
    "\n",
    "education_order = df[\"education\"].value_counts().index\n",
    "sns.countplot(data=df, y=\"education\", hue=\"income\", order=education_order, ax=axes[1])\n",
    "axes[1].set_title(\"Répartition de l'income par niveau d'éducation\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3a666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True)\n",
    "plt.title(\"Heatmap des corrélations (variables numériques)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17223e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "\n",
    "numeric_data = df[numeric_cols].copy()\n",
    "numeric_scaled = scaler.fit_transform(numeric_data)\n",
    "pca_components = pca.fit_transform(numeric_scaled)\n",
    "\n",
    "pca_df = pd.DataFrame(pca_components, columns=[\"PC1\", \"PC2\"])\n",
    "pca_df[\"income\"] = df[\"income\"].values\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=pca_df, x=\"PC1\", y=\"PC2\", hue=\"income\", alpha=0.5)\n",
    "plt.title(\"PCA (2 composantes) colorée par income\")\n",
    "plt.show()\n",
    "\n",
    "explained_var = pca.explained_variance_ratio_\n",
    "print(f\"Variance expliquée par PC1+PC2: {explained_var.sum():.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c789c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(numeric_scaled)\n",
    "\n",
    "cluster_df = pd.DataFrame(pca_components, columns=[\"PC1\", \"PC2\"])\n",
    "cluster_df[\"cluster\"] = clusters.astype(str)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=cluster_df, x=\"PC1\", y=\"PC2\", hue=\"cluster\", palette=\"tab10\", alpha=0.6)\n",
    "plt.title(\"Clusters K-Means projetés sur les composantes PCA\")\n",
    "plt.show()\n",
    "\n",
    "cluster_target = pd.crosstab(clusters, df[\"income\"], normalize=\"index\")\n",
    "print(\"Distribution de income par cluster:\")\n",
    "print(cluster_target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c543731",
   "metadata": {},
   "source": [
    "# Préparation des données & Découpage Train/Test\n",
    "\n",
    "Nous encodons la cible, séparons features/cible, puis construisons un pipeline de prétraitement combinant imputations, standardisation des numériques et encodage one-hot des variables catégorielles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b02e92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"income\"\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"income_encoded\"] = label_encoder.fit_transform(df[target_col])\n",
    "\n",
    "y = df[\"income_encoded\"]\n",
    "X = df.drop(columns=[target_col, \"income_encoded\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981a47d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\n",
    "    \"age\",\n",
    "    \"fnlwgt\",\n",
    "    \"educational-num\",\n",
    "    \"capital-gain\",\n",
    "    \"capital-loss\",\n",
    "    \"hours-per-week\",\n",
    "]\n",
    "\n",
    "categorical_features = [col for col in X.columns if col not in numeric_features]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Variables numériques ({len(numeric_features)}): {numeric_features}\")\n",
    "print(f\"Variables catégorielles ({len(categorical_features)}): {categorical_features}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6a7776",
   "metadata": {},
   "source": [
    "## Partie 3 · Modélisation : Baseline vs Ensembles\n",
    "\n",
    "Nous entraînons trois modèles (Logistic Regression, Random Forest, Gradient Boosting) encapsulés dans un pipeline complet, évaluons leurs performances via des rapports de classification, comparons Accuracy/F1, puis inspectons la matrice de confusion du meilleur modèle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c6c4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "}\n",
    "\n",
    "trained_pipelines = {}\n",
    "metrics_records = []\n",
    "\n",
    "for name, estimator in models.items():\n",
    "    clf = Pipeline(steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"model\", estimator),\n",
    "    ])\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    metrics_records.append({\"Model\": name, \"Accuracy\": acc, \"F1-Score\": f1})\n",
    "    trained_pipelines[name] = clf\n",
    "\n",
    "    print(f\"===== {name} =====\")\n",
    "    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_records).sort_values(by=\"F1-Score\", ascending=False)\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45579453",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = metrics_df.iloc[0][\"Model\"]\n",
    "best_pipeline = trained_pipelines[best_model_name]\n",
    "y_pred_best = best_pipeline.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel(\"Prédictions\")\n",
    "plt.ylabel(\"Vérité terrain\")\n",
    "plt.title(f\"Matrice de confusion · {best_model_name}\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Meilleur modèle selon F1: {best_model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74186de8",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Les modèles d'ensemble (Random Forest et Gradient Boosting) surpassent la régression logistique sur l'Accuracy et le F1-Score, le meilleur selon F1 étant affiché dans la matrice de confusion. Cette configuration offre une base solide pour de futures itérations (optimisation d'hyperparamètres, gestion du déséquilibre, interprétabilité).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
