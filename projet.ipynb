{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b70c21d4",
   "metadata": {},
   "source": [
    "# Adult Income Classification Project\n",
    "\n",
    "Le projet vise √† analyser le jeu de donn√©es Adult Income afin d‚Äôidentifier les diff√©rents facteurs qui influencent le niveau de revenu d'une personne et de construire un mod√®le capable de pr√©dire si une personne gagne plus ou moins de 50 000 dollars par an. \n",
    "\n",
    "Le dataset, disponible dans plusieurs versions (UCI, OpenML, variantes nettoy√©es ou simplifi√©es), est ici utilis√© dans sa version OpenML, qui offre un format homog√®ne et des variables √† la fois num√©riques et cat√©gorielles. Notre probl√®me sera approch√© sous la forme d'une classification binaire (+ ou - de 50k$/an de revenus) car le dataset ne contient pas la valeur exacte du revenu, qu'une cat√©gorie.\n",
    "\n",
    "Ce notebook sera articul√© en trois grandes parties :\n",
    "\n",
    "1. **Exploration des donn√©es (EDA) et analyses non supervis√©es**\n",
    "2. **Pr√©processing des donn√©es et s√©paration des jeux d'apprentissage/test**\n",
    "3. **Mod√©lisation supervis√©e (baseline vs ensembles) et comparaison des performances**\n",
    "\n",
    "Chaque section inclut des explications d√©taill√©es pour assurer la reproductibilit√© de l'analyse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0924681d",
   "metadata": {},
   "source": [
    "Avant de commencer, voici tous les imports de librairie dont nous auront besoin pour lancer les cellules ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b62386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", context=\"notebook\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 5)\n",
    "\n",
    "DATA_PATH = \"adult.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e0139b",
   "metadata": {},
   "source": [
    "# 1 - Exploration de donn√©es et analyse non supervis√©e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d870bb3c",
   "metadata": {},
   "source": [
    "## Caract√©ristiques du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cf39cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9283f935",
   "metadata": {},
   "source": [
    "**Nombre d'instances :** Le dataset complet contient 48 842 entr√©es. C'est un volume suffisant pour entra√Æner un mod√®le robuste.\n",
    "\n",
    "**Nombre de features :** Il y a 14 variables explicatives + 1 variable cible (income)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5310451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTypes de donn√©es:\")\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69747d21",
   "metadata": {},
   "source": [
    "**Types de variables :**\n",
    "\n",
    "- **Num√©riques :** age, fnlwgt (= final weight), education-num, capital-gain, capital-loss, hours-per-week.\n",
    "\n",
    "- **Cat√©gorielles :** workclass, education, marital-status, occupation, relationship, race, sex, native-country."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd8a04e",
   "metadata": {},
   "source": [
    "## Analyse et gestion des valeurs manquantes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3068427",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nValeurs manquantes par colonne:\")\n",
    "missing_counts = df.isna().sum()\n",
    "print(missing_counts[missing_counts > 0] if missing_counts.sum() > 0 else \"Aucune valeur manquante d√©tect√©e\")\n",
    "\n",
    "# D√©tecter les '?' comme valeurs manquantes\n",
    "print(\"\\n--- D√©tection des '?' comme valeurs manquantes ---\")\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        question_count = (df[col] == '?').sum()\n",
    "        if question_count > 0:\n",
    "            print(f\"{col}: {question_count} valeurs manquantes ('?')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d60501a",
   "metadata": {},
   "source": [
    "\n",
    "### Interpr√©tation des valeurs manquantes\n",
    "\n",
    "Les points d'interrogation ('?') observ√©s dans le dataset repr√©sentent des **valeurs manquantes** (donn√©es non disponibles ou non renseign√©es). Ces valeurs peuvent affecter la qualit√© de nos pr√©dictions et doivent √™tre trait√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7686ec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace('?', np.nan)\n",
    "\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Colonne': df.columns,\n",
    "    'Manquantes': df.isna().sum(),\n",
    "    'Pourcentage': (df.isna().sum() / len(df) * 100).round(2)\n",
    "})\n",
    "missing_summary = missing_summary[missing_summary['Manquantes'] > 0].sort_values('Manquantes', ascending=False)\n",
    "print(missing_summary.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d881446",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df.copy()\n",
    "df_temp = df_temp.replace('?', np.nan)\n",
    "rows_with_missing = df_temp.isna().any(axis=1).sum()\n",
    "\n",
    "total_rows = len(df)\n",
    "percentage_lost = (rows_with_missing / total_rows) * 100\n",
    "\n",
    "print(f\"   - Nombre total de lignes: {total_rows:,}\")\n",
    "print(f\"   - Lignes contenant au moins un '?': {rows_with_missing:,}\")\n",
    "print(f\"   - Pourcentage de donn√©es PERDUES: {percentage_lost:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44f8f46",
   "metadata": {},
   "source": [
    "Notre analyse pr√©liminaire montre que la suppression des lignes contenant au moins une valeur manquante entra√Ænerait une perte de donn√©es sup√©rieure √† 5%. Nous craignons que r√©duire la taille du jeu d'entra√Ænement d'autant de valeurs puisse nuire √† la capacit√© du mod√®le √† g√©n√©raliser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20352062",
   "metadata": {},
   "source": [
    "### Strat√©gie choisie pour ce projet\n",
    "\n",
    "Nous n'avons que des variables cat√©gorielles √† imputer, nous avons donc fait le choix de faire une imputation par mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3561612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application de l'imputation par mode (most_frequent) sur les variables cat√©gorielles\n",
    "print(\"=== Avant imputation ===\")\n",
    "print(f\"Valeurs manquantes totales : {df.isna().sum().sum()}\")\n",
    "print(\"\\nD√©tail par colonne :\")\n",
    "print(df.isna().sum()[df.isna().sum() > 0])\n",
    "\n",
    "# Imputation des variables cat√©gorielles avec la valeur la plus fr√©quente (mode)\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    if df[col].isna().sum() > 0:\n",
    "        mode_value = df[col].mode()[0]  # R√©cup√©ration du mode\n",
    "        df[col].fillna(mode_value, inplace=True)\n",
    "        print(f\"\\n - {col} : imput√© avec '{mode_value}'\")\n",
    "\n",
    "print(\"\\n=== Apr√®s imputation ===\")\n",
    "print(f\"Valeurs manquantes totales : {df.isna().sum().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f5c5b4",
   "metadata": {},
   "source": [
    "## Analyse des distributions\n",
    "\n",
    "Cette section examine la r√©partition des valeurs pour chaque variable du dataset, ce qui permet de :\n",
    "- Comprendre la forme des distributions\n",
    "- Identifier les d√©s√©quilibres dans les variables cat√©gorielles\n",
    "- D√©tecter d'√©ventuels patterns ou anomalies dans les donn√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae743c8c",
   "metadata": {},
   "source": [
    "### Distribution des variables num√©riques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2270d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogrammes des variables num√©riques\n",
    "numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, col in enumerate(numeric_cols):\n",
    "    axes[idx].hist(df[col], bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f\"Distribution de {col}\", fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel(\"Fr√©quence\")\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460c86cc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Observations :**\n",
    "- **age** : Distribution uniforme, pic 30-40 ans (population active)\n",
    "- **fnlwgt** : Poids d√©mographique Census (~200k). Candidate √† la suppression (pond√©ration statistique, non pr√©dictive)\n",
    "- **educational-num** : Concentr√© 9-10 (HS-grad/Some-college)\n",
    "- **capital-gain/loss** : ~95% de z√©ros, valeurs √©lev√©es rares ‚Üí discriminant pour hauts revenus\n",
    "- **hours-per-week** : Pic √† 40h (temps plein), quelques extr√™mes (>60h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff91318f",
   "metadata": {},
   "source": [
    "### Distribution des variables cat√©gorielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85047296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©lection des principales variables cat√©gorielles √† visualiser\n",
    "categorical_cols = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'gender']\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(16, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, col in enumerate(categorical_cols):\n",
    "    value_counts = df[col].value_counts()\n",
    "    axes[idx].barh(value_counts.index, value_counts.values, color='coral', edgecolor='black')\n",
    "    axes[idx].set_title(f\"Distribution de {col}\", fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(\"Fr√©quence\")\n",
    "    axes[idx].invert_yaxis()\n",
    "    \n",
    "    # Afficher les pourcentages\n",
    "    for i, v in enumerate(value_counts.values):\n",
    "        axes[idx].text(v + max(value_counts.values)*0.01, i, f'{v} ({v/len(df)*100:.1f}%)', \n",
    "                       va='center', fontsize=9)\n",
    "\n",
    "# Masquer le dernier subplot s'il est vide\n",
    "if len(categorical_cols) < len(axes):\n",
    "    axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c596d7",
   "metadata": {},
   "source": [
    "**Observations :**\n",
    "- **workclass** : Private dominant (~75%), Self-employed ~10%, secteur public ~15%\n",
    "- **education** : HS-grad et Some-college majoritaires, Bachelors ~16%, Masters ~5%\n",
    "- **marital-status** : Married-civ-spouse ~45%, Never-married ~35% ‚Üí tr√®s pr√©dictif (doubles revenus)\n",
    "- **occupation** : Distribution √©quilibr√©e, Prof-specialty et Craft-repair dominants\n",
    "- **relationship** : Husband ~40%, Not-in-family ~26% ‚Üí Risque colin√©arit√© avec marital-status\n",
    "- **race** : White ~85%, Black ~10% ‚Üí Variable sensible (biais potentiels)\n",
    "- **gender** : Male ~67%, Female ~33% ‚Üí Variable sensible (biais de genre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23776ae7",
   "metadata": {},
   "source": [
    "## Distributions, Outliers & Pr√©paration\n",
    "\n",
    "**Variables \"tail-heavy\" (asym√©trie forte) :**\n",
    "- capital-gain/loss : ~95% z√©ros, valeurs √©lev√©es rares (>20k$)\n",
    "- fnlwgt : Longue queue droite\n",
    "- hours-per-week : Centr√©e 40h, extr√™mes >80h\n",
    "\n",
    "**√âchelles h√©t√©rog√®nes :**\n",
    "- fnlwgt : 10k-1M (x100) | capital-gain/loss : 0-100k$ (x1000) | age : 17-90\n",
    "\n",
    "‚Üí N√©cessite normalisation pour √©viter domination artificielle dans les algorithmes √† distance/gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4da141",
   "metadata": {},
   "source": [
    "### D√©tection des Outliers : Isolation Forest\n",
    "\n",
    "**Choix :** Isolation Forest pour d√©tecter anomalies multidimensionnelles\n",
    "- Adapt√© aux distributions asym√©triques\n",
    "- Aucune hypoth√®se sur la distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c678c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# S√©lection des variables num√©riques pour la d√©tection d'outliers\n",
    "numeric_cols_for_outliers = ['age', 'fnlwgt', 'educational-num', \n",
    "                              'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "\n",
    "# Cr√©ation d'un subset sans valeurs manquantes pour l'analyse\n",
    "df_numeric = df[numeric_cols_for_outliers].copy()\n",
    "\n",
    "# Remplacement des '?' par NaN si pr√©sents (bien que peu probable pour les num√©riques)\n",
    "# Supprimer les lignes avec NaN pour Isolation Forest\n",
    "df_numeric_clean = df_numeric.dropna()\n",
    "\n",
    "print(f\"Dataset pour d√©tection d'outliers: {df_numeric_clean.shape[0]} observations\")\n",
    "\n",
    "# Isolation Forest avec contamination=0.05 (on estime 5% d'outliers)\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42, n_jobs=-1)\n",
    "outlier_predictions = iso_forest.fit_predict(df_numeric_clean)\n",
    "\n",
    "# -1 = outlier, 1 = inlier\n",
    "n_outliers = (outlier_predictions == -1).sum()\n",
    "outlier_percentage = (n_outliers / len(outlier_predictions)) * 100\n",
    "\n",
    "print(f\"\\nüìä R√©sultats de la d√©tection d'outliers:\")\n",
    "print(f\"   - Outliers d√©tect√©s: {n_outliers} ({outlier_percentage:.2f}%)\")\n",
    "print(f\"   - Inliers (normaux): {(outlier_predictions == 1).sum()} ({100-outlier_percentage:.2f}%)\")\n",
    "\n",
    "# Analyse des caract√©ristiques des outliers\n",
    "df_numeric_clean['is_outlier'] = outlier_predictions == -1\n",
    "outlier_summary = df_numeric_clean.groupby('is_outlier')[numeric_cols_for_outliers].mean()\n",
    "\n",
    "print(\"\\nüìà Comparaison Inliers vs Outliers (moyennes):\")\n",
    "print(outlier_summary.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d1ba95",
   "metadata": {},
   "source": [
    "**Interpr√©tation des moyennes :**\n",
    "\n",
    "Les outliers pr√©sentent des profils distincts r√©v√©lant leur nature :\n",
    "- **Age +9 ans** (47 vs 38) ‚Üí Population plus mature\n",
    "- **capital-gain x26** (12,440$ vs 480$) ‚Üí Revenus de capitaux √©lev√©s\n",
    "- **capital-loss x44** (1,221$ vs 28$) ‚Üí Pertes en capital significatives\n",
    "- **hours-per-week +5h** (45 vs 40) ‚Üí Plus d'heures travaill√©es\n",
    "\n",
    "Cependant, ces outliers repr√©sentent des profils r√©els (cadres seniors, investisseurs) et non des erreurs de mesure.\n",
    "\n",
    "**M√©thode de gestion : Conservation + Double att√©nuation**\n",
    "\n",
    "1. **Pas de suppression** : Profils l√©gitimes, informations utiles pour le mod√®le\n",
    "2. **Att√©nuation automatique via StandardScaler** : Ram√®ne valeurs extr√™mes vers 0, r√©duit leur influence\n",
    "3. **Robustesse intrins√®que** : Random Forest/Gradient Boosting tol√®rent naturellement les outliers (splits binaires, pas de calculs de distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f85b3d",
   "metadata": {},
   "source": [
    "### Distribution de la variable cible (Target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16dc259",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(data=df, x=\"income\")\n",
    "plt.title(\"R√©partition de la variable cible (income)\")\n",
    "plt.show()\n",
    "\n",
    "income_ratio = df[\"income\"].value_counts(normalize=True)\n",
    "print(\"R√©partition proportionnelle:\\n\", income_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0da26de",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "sns.boxplot(data=df, x=\"income\", y=\"age\", ax=axes[0])\n",
    "axes[0].set_title(\"Distribution de l'√¢ge selon le revenu\")\n",
    "\n",
    "education_order = df[\"education\"].value_counts().index\n",
    "sns.countplot(data=df, y=\"education\", hue=\"income\", order=education_order, ax=axes[1])\n",
    "axes[1].set_title(\"R√©partition de l'income par niveau d'√©ducation\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3a666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True)\n",
    "plt.title(\"Heatmap des corr√©lations (variables num√©riques)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17223e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "\n",
    "numeric_data = df[numeric_cols].copy()\n",
    "numeric_scaled = scaler.fit_transform(numeric_data)\n",
    "pca_components = pca.fit_transform(numeric_scaled)\n",
    "\n",
    "pca_df = pd.DataFrame(pca_components, columns=[\"PC1\", \"PC2\"])\n",
    "pca_df[\"income\"] = df[\"income\"].values\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=pca_df, x=\"PC1\", y=\"PC2\", hue=\"income\", alpha=0.5)\n",
    "plt.title(\"PCA (2 composantes) color√©e par income\")\n",
    "plt.show()\n",
    "\n",
    "explained_var = pca.explained_variance_ratio_\n",
    "print(f\"Variance expliqu√©e par PC1+PC2: {explained_var.sum():.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c789c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(numeric_scaled)\n",
    "\n",
    "cluster_df = pd.DataFrame(pca_components, columns=[\"PC1\", \"PC2\"])\n",
    "cluster_df[\"cluster\"] = clusters.astype(str)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=cluster_df, x=\"PC1\", y=\"PC2\", hue=\"cluster\", palette=\"tab10\", alpha=0.6)\n",
    "plt.title(\"Clusters K-Means projet√©s sur les composantes PCA\")\n",
    "plt.show()\n",
    "\n",
    "cluster_target = pd.crosstab(clusters, df[\"income\"], normalize=\"index\")\n",
    "print(\"Distribution de income par cluster:\")\n",
    "print(cluster_target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c543731",
   "metadata": {},
   "source": [
    "# Pr√©paration des donn√©es & D√©coupage Train/Test\n",
    "\n",
    "Nous encodons la cible, s√©parons features/cible, puis construisons un pipeline de pr√©traitement combinant imputations, standardisation des num√©riques et encodage one-hot des variables cat√©gorielles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b02e92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"income\"\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"income_encoded\"] = label_encoder.fit_transform(df[target_col])\n",
    "\n",
    "y = df[\"income_encoded\"]\n",
    "X = df.drop(columns=[target_col, \"income_encoded\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981a47d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\n",
    "    \"age\",\n",
    "    \"fnlwgt\",\n",
    "    \"educational-num\",\n",
    "    \"capital-gain\",\n",
    "    \"capital-loss\",\n",
    "    \"hours-per-week\",\n",
    "]\n",
    "\n",
    "categorical_features = [col for col in X.columns if col not in numeric_features]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Variables num√©riques ({len(numeric_features)}): {numeric_features}\")\n",
    "print(f\"Variables cat√©gorielles ({len(categorical_features)}): {categorical_features}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6a7776",
   "metadata": {},
   "source": [
    "# 3 - Mod√©lisation : Baseline vs Ensembles\n",
    "\n",
    "Entra√Ænement de 3 mod√®les (Logistic Regression, Random Forest, Gradient Boosting) avec pipeline complet. Comparaison Accuracy/F1 et matrice de confusion du meilleur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c6c4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "}\n",
    "\n",
    "trained_pipelines = {}\n",
    "metrics_records = []\n",
    "\n",
    "for name, estimator in models.items():\n",
    "    clf = Pipeline(steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"model\", estimator),\n",
    "    ])\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    metrics_records.append({\"Model\": name, \"Accuracy\": acc, \"F1-Score\": f1})\n",
    "    trained_pipelines[name] = clf\n",
    "\n",
    "    print(f\"===== {name} =====\")\n",
    "    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_records).sort_values(by=\"F1-Score\", ascending=False)\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45579453",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = metrics_df.iloc[0][\"Model\"]\n",
    "best_pipeline = trained_pipelines[best_model_name]\n",
    "y_pred_best = best_pipeline.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel(\"Pr√©dictions\")\n",
    "plt.ylabel(\"V√©rit√© terrain\")\n",
    "plt.title(f\"Matrice de confusion ¬∑ {best_model_name}\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Meilleur mod√®le selon F1: {best_model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74186de8",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Mod√®les d'ensemble (RF, GB) surpassent Logistic Regression. Base solide pour it√©rations futures (hyperparam√®tres, d√©s√©quilibre, interpr√©tabilit√©)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
